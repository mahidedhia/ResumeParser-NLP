{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahidedhia/ResumeParser-NLP/blob/master/ResumeParsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries"
      ],
      "metadata": {
        "id": "3UUA0TtcIkNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n",
        "!pip install pretty_html_table\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e656e74c-3d88-45d8-8b1e-08a845cb5bc6",
        "id": "_HuGZ1MrudY1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.9/dist-packages (20221105)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (40.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pretty_html_table in /usr/local/lib/python3.9/dist-packages (0.9.16)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from pretty_html_table) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->pretty_html_table) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas->pretty_html_table) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->pretty_html_table) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->pretty_html_table) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7NJ9Bl77udY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9815cd21-48a8-446a-caa1-084d6b36ed9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import io\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import dateutil\n",
        "import re\n",
        "import json\n",
        "from pretty_html_table import build_table\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#for stopwords required for preprocessing\n",
        "stopwords = nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PDF to Text Conversion"
      ],
      "metadata": {
        "id": "QXggxfn8IUVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"QA-Manual-Tester.pdf\""
      ],
      "metadata": {
        "id": "4HRY-zIQvXSq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        # iterate over all pages of PDF document\n",
        "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
        "            # creating a resoure manager\n",
        "            resource_manager = PDFResourceManager()\n",
        "            \n",
        "            # create a file handle\n",
        "            fake_file_handle = io.StringIO()\n",
        "            \n",
        "            # creating a text converter object\n",
        "            converter = TextConverter(\n",
        "                                resource_manager, \n",
        "                                fake_file_handle, \n",
        "                                codec='utf-8', \n",
        "                                laparams=LAParams()\n",
        "                        )\n",
        "\n",
        "            # creating a page interpreter\n",
        "            page_interpreter = PDFPageInterpreter(\n",
        "                                resource_manager, \n",
        "                                converter\n",
        "                            )\n",
        "\n",
        "            # process current page\n",
        "            page_interpreter.process_page(page)\n",
        "            \n",
        "            # extract text\n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "\n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()"
      ],
      "metadata": {
        "id": "BHHVg78VJF14"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = ''\n",
        "# calling above function and extracting text\n",
        "for page in extract_text_from_pdf(filename):\n",
        "    resume_text += page + ' '\n",
        "\n",
        "resume_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "m5NssPB8JOCM",
        "outputId": "0c15f99c-546b-4664-b50d-299474e1913c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FIRST LAST\\nBay Area, California • +1-234-456-789 • professionalemail@resumeworded.com • linkedin.com/in/username\\n\\nPROFESSIONAL EXPERIENCE\\n\\nResume Worded, New York, NY\\nQA Manual Tester\\n\\nJun 2018 – Present\\n\\n● Enabled critical test case complexity metrics with support for Rapid adoption of  functional automation using\\n\\na scriptless test case adaptor by standardizing a Test Case construction method that was built\\nAutomation-ready & supported a test automation framework leading to a 45% increase in reusability with\\nreductions in TCO approaching 25%.\\n\\n● Optimized scripting, modularity, & maintenance which resulted in an 18% decrease in workflow friction.\\n● Increased the company’s ability to take and complete projects without increasing manpower by 15% by\\n\\nreducing QA testing turnaround time by 30%.\\n\\nGrowthsi, New York, NY\\nQA Manual Tester\\n\\nJan 2015 – May 2018\\n\\n● Restructured utilities & improved the process documentation leading to a 40% reduction in client support\\n\\ntickets & an 80% increase in uptime.\\n\\n● Achieved department-wide improvement metrics based on QA scorecard through the 46% & 22% workload\\n\\nreduction of  the customer support & IT departments respectively.\\n\\n● Standardized Test Plan, Test Scripts/Test Cases, Daily Status Reports, etc., documents leading to a 20%\\n\\nincrease in productivity.\\n\\n● Established monthly sprint backlog items as well as performed agile meetings while updating the activities in\\n\\nMicrosoft TFS in an optimized manner which resulted in saving 10 hours of  monthly lost time.\\n\\nRW Capital, San Diego, CA\\nQA Manual Tester (Nov 2011 – Dec 2014)\\n\\nMay 2008 – Dec 2014\\n\\n● Optimized the build process by increasing the system’s quality level and reducing 45% of  defects found.\\n● Established proper team communication that identified, triaged, reproduced, & fixed found issues using JIRA\\n\\nincreasing the overall workflow by 25%.\\n\\nJunior QA Manual Tester (May 2010 – Oct 2011)\\n\\n● Wrote & optimized test scripts in towels which led to a 9% reduction in the overall testing hours.\\n● Created traceability matrix to fill in the gap between requirements and tests covered contributing to the 10%\\n\\nincrease in test case count.\\n\\nEDUCATION\\n\\nResume Worded University, San Francisco, CA\\nBSc. Computer Science\\n\\nSKILLS\\n\\n● Test Automation\\nFrameworks\\n\\n● Java\\n● Javascript\\n\\n● Microsoft TFS\\n● CharlesProxy\\nSQL/NoSQL\\n●\\nSelenium/Webdriver\\n\\n●\\n\\n● TestNG\\nJIRA\\n●\\nJenkins\\n\\n●\\n● Agile\\n\\nMay 2010\\n\\n●\\n\\n●\\n\\nSource Versioning\\nJUnit\\nScrum\\n\\n●\\n● …\\n\\n\\x0c '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing unwanted unicode characters\n",
        "resume_text = resume_text.encode(\"ascii\", \"ignore\")\n",
        "resume_text = resume_text.decode()\n",
        "resume_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "Ui0R_cn0Q9hY",
        "outputId": "9f42ad38-3f83-4d84-dddb-5175609cf46b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FIRST LAST\\nBay Area, California  +1-234-456-789  professionalemail@resumeworded.com  linkedin.com/in/username\\n\\nPROFESSIONAL EXPERIENCE\\n\\nResume Worded, New York, NY\\nQA Manual Tester\\n\\nJun 2018  Present\\n\\n Enabled critical test case complexity metrics with support for Rapid adoption of  functional automation using\\n\\na scriptless test case adaptor by standardizing a Test Case construction method that was built\\nAutomation-ready & supported a test automation framework leading to a 45% increase in reusability with\\nreductions in TCO approaching 25%.\\n\\n Optimized scripting, modularity, & maintenance which resulted in an 18% decrease in workflow friction.\\n Increased the companys ability to take and complete projects without increasing manpower by 15% by\\n\\nreducing QA testing turnaround time by 30%.\\n\\nGrowthsi, New York, NY\\nQA Manual Tester\\n\\nJan 2015  May 2018\\n\\n Restructured utilities & improved the process documentation leading to a 40% reduction in client support\\n\\ntickets & an 80% increase in uptime.\\n\\n Achieved department-wide improvement metrics based on QA scorecard through the 46% & 22% workload\\n\\nreduction of  the customer support & IT departments respectively.\\n\\n Standardized Test Plan, Test Scripts/Test Cases, Daily Status Reports, etc., documents leading to a 20%\\n\\nincrease in productivity.\\n\\n Established monthly sprint backlog items as well as performed agile meetings while updating the activities in\\n\\nMicrosoft TFS in an optimized manner which resulted in saving 10 hours of  monthly lost time.\\n\\nRW Capital, San Diego, CA\\nQA Manual Tester (Nov 2011  Dec 2014)\\n\\nMay 2008  Dec 2014\\n\\n Optimized the build process by increasing the systems quality level and reducing 45% of  defects found.\\n Established proper team communication that identified, triaged, reproduced, & fixed found issues using JIRA\\n\\nincreasing the overall workflow by 25%.\\n\\nJunior QA Manual Tester (May 2010  Oct 2011)\\n\\n Wrote & optimized test scripts in towels which led to a 9% reduction in the overall testing hours.\\n Created traceability matrix to fill in the gap between requirements and tests covered contributing to the 10%\\n\\nincrease in test case count.\\n\\nEDUCATION\\n\\nResume Worded University, San Francisco, CA\\nBSc. Computer Science\\n\\nSKILLS\\n\\n Test Automation\\nFrameworks\\n\\n Java\\n Javascript\\n\\n Microsoft TFS\\n CharlesProxy\\nSQL/NoSQL\\n\\nSelenium/Webdriver\\n\\n\\n\\n TestNG\\nJIRA\\n\\nJenkins\\n\\n\\n Agile\\n\\nMay 2010\\n\\n\\n\\n\\n\\nSource Versioning\\nJUnit\\nScrum\\n\\n\\n \\n\\n\\x0c '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-Processing"
      ],
      "metadata": {
        "id": "_obkh4NPJkWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for tokenizing\n",
        "def tokenize(text):\n",
        "  #using spacy\n",
        "  doc = nlp(text)\n",
        "  word_tokens = []\n",
        "  for token in doc:\n",
        "    word_tokens.append(str(token))\n",
        "  return word_tokens"
      ],
      "metadata": {
        "id": "TwS-hsJWSFsE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for removing stop words and special characters. also converting tokens to lowercase\n",
        "def stop_words(word_tokens):\n",
        "  #using nltk\n",
        "  special_char='\\\"[,@_!$%^&*()<>?/\\|}{~:]#.-\\''\n",
        "  removed_stop_words = [word.lower() for word in word_tokens if word not in stops and str(word) not in special_char and not (ord(str(word)[0]) >= 0 and ord(str(word)[0]) <= 32)] \n",
        "  return removed_stop_words"
      ],
      "metadata": {
        "id": "M8OWsGyNToau"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessess(text):\n",
        "  #removing \\n\n",
        "  text = text.replace('\\n', ' ')\n",
        "  #word tokenzing\n",
        "  word_tokens=tokenize(text)\n",
        "  #removal of stopwords and special characters\n",
        "  removed_stop_words = stop_words(word_tokens)\n",
        "  return removed_stop_words"
      ],
      "metadata": {
        "id": "i601Kbo7RPHO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preProcessedTokens = preProcessess(resume_text)\n",
        "preProcessedTokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m99_ZYtsVjT_",
        "outputId": "81af592e-ef85-4dd9-9075-30f92acaf368"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first',\n",
              " 'last',\n",
              " 'bay',\n",
              " 'area',\n",
              " 'california',\n",
              " '+1',\n",
              " '234',\n",
              " '456',\n",
              " '789',\n",
              " 'professionalemail@resumeworded.com',\n",
              " 'linkedin.com/in/username',\n",
              " 'professional',\n",
              " 'experience',\n",
              " 'resume',\n",
              " 'worded',\n",
              " 'new',\n",
              " 'york',\n",
              " 'ny',\n",
              " 'qa',\n",
              " 'manual',\n",
              " 'tester',\n",
              " 'jun',\n",
              " '2018',\n",
              " 'present',\n",
              " 'enabled',\n",
              " 'critical',\n",
              " 'test',\n",
              " 'case',\n",
              " 'complexity',\n",
              " 'metrics',\n",
              " 'support',\n",
              " 'rapid',\n",
              " 'adoption',\n",
              " 'functional',\n",
              " 'automation',\n",
              " 'using',\n",
              " 'scriptless',\n",
              " 'test',\n",
              " 'case',\n",
              " 'adaptor',\n",
              " 'standardizing',\n",
              " 'test',\n",
              " 'case',\n",
              " 'construction',\n",
              " 'method',\n",
              " 'built',\n",
              " 'automation',\n",
              " 'ready',\n",
              " 'supported',\n",
              " 'test',\n",
              " 'automation',\n",
              " 'framework',\n",
              " 'leading',\n",
              " '45',\n",
              " 'increase',\n",
              " 'reusability',\n",
              " 'reductions',\n",
              " 'tco',\n",
              " 'approaching',\n",
              " '25',\n",
              " 'optimized',\n",
              " 'scripting',\n",
              " 'modularity',\n",
              " 'maintenance',\n",
              " 'resulted',\n",
              " '18',\n",
              " 'decrease',\n",
              " 'workflow',\n",
              " 'friction',\n",
              " 'increased',\n",
              " 'companys',\n",
              " 'ability',\n",
              " 'take',\n",
              " 'complete',\n",
              " 'projects',\n",
              " 'without',\n",
              " 'increasing',\n",
              " 'manpower',\n",
              " '15',\n",
              " 'reducing',\n",
              " 'qa',\n",
              " 'testing',\n",
              " 'turnaround',\n",
              " 'time',\n",
              " '30',\n",
              " 'growthsi',\n",
              " 'new',\n",
              " 'york',\n",
              " 'ny',\n",
              " 'qa',\n",
              " 'manual',\n",
              " 'tester',\n",
              " 'jan',\n",
              " '2015',\n",
              " 'may',\n",
              " '2018',\n",
              " 'restructured',\n",
              " 'utilities',\n",
              " 'improved',\n",
              " 'process',\n",
              " 'documentation',\n",
              " 'leading',\n",
              " '40',\n",
              " 'reduction',\n",
              " 'client',\n",
              " 'support',\n",
              " 'tickets',\n",
              " '80',\n",
              " 'increase',\n",
              " 'uptime',\n",
              " 'achieved',\n",
              " 'department',\n",
              " 'wide',\n",
              " 'improvement',\n",
              " 'metrics',\n",
              " 'based',\n",
              " 'qa',\n",
              " 'scorecard',\n",
              " '46',\n",
              " '22',\n",
              " 'workload',\n",
              " 'reduction',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'it',\n",
              " 'departments',\n",
              " 'respectively',\n",
              " 'standardized',\n",
              " 'test',\n",
              " 'plan',\n",
              " 'test',\n",
              " 'scripts',\n",
              " 'test',\n",
              " 'cases',\n",
              " 'daily',\n",
              " 'status',\n",
              " 'reports',\n",
              " 'etc',\n",
              " 'documents',\n",
              " 'leading',\n",
              " '20',\n",
              " 'increase',\n",
              " 'productivity',\n",
              " 'established',\n",
              " 'monthly',\n",
              " 'sprint',\n",
              " 'backlog',\n",
              " 'items',\n",
              " 'well',\n",
              " 'performed',\n",
              " 'agile',\n",
              " 'meetings',\n",
              " 'updating',\n",
              " 'activities',\n",
              " 'microsoft',\n",
              " 'tfs',\n",
              " 'optimized',\n",
              " 'manner',\n",
              " 'resulted',\n",
              " 'saving',\n",
              " '10',\n",
              " 'hours',\n",
              " 'monthly',\n",
              " 'lost',\n",
              " 'time',\n",
              " 'rw',\n",
              " 'capital',\n",
              " 'san',\n",
              " 'diego',\n",
              " 'ca',\n",
              " 'qa',\n",
              " 'manual',\n",
              " 'tester',\n",
              " 'nov',\n",
              " '2011',\n",
              " 'dec',\n",
              " '2014',\n",
              " 'may',\n",
              " '2008',\n",
              " 'dec',\n",
              " '2014',\n",
              " 'optimized',\n",
              " 'build',\n",
              " 'process',\n",
              " 'increasing',\n",
              " 'systems',\n",
              " 'quality',\n",
              " 'level',\n",
              " 'reducing',\n",
              " '45',\n",
              " 'defects',\n",
              " 'found',\n",
              " 'established',\n",
              " 'proper',\n",
              " 'team',\n",
              " 'communication',\n",
              " 'identified',\n",
              " 'triaged',\n",
              " 'reproduced',\n",
              " 'fixed',\n",
              " 'found',\n",
              " 'issues',\n",
              " 'using',\n",
              " 'jira',\n",
              " 'increasing',\n",
              " 'overall',\n",
              " 'workflow',\n",
              " '25',\n",
              " 'junior',\n",
              " 'qa',\n",
              " 'manual',\n",
              " 'tester',\n",
              " 'may',\n",
              " '2010',\n",
              " 'oct',\n",
              " '2011',\n",
              " 'wrote',\n",
              " 'optimized',\n",
              " 'test',\n",
              " 'scripts',\n",
              " 'towels',\n",
              " 'led',\n",
              " '9',\n",
              " 'reduction',\n",
              " 'overall',\n",
              " 'testing',\n",
              " 'hours',\n",
              " 'created',\n",
              " 'traceability',\n",
              " 'matrix',\n",
              " 'fill',\n",
              " 'gap',\n",
              " 'requirements',\n",
              " 'tests',\n",
              " 'covered',\n",
              " 'contributing',\n",
              " '10',\n",
              " 'increase',\n",
              " 'test',\n",
              " 'case',\n",
              " 'count',\n",
              " 'education',\n",
              " 'resume',\n",
              " 'worded',\n",
              " 'university',\n",
              " 'san',\n",
              " 'francisco',\n",
              " 'ca',\n",
              " 'bsc',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'skills',\n",
              " 'test',\n",
              " 'automation',\n",
              " 'frameworks',\n",
              " 'java',\n",
              " 'javascript',\n",
              " 'microsoft',\n",
              " 'tfs',\n",
              " 'charlesproxy',\n",
              " 'sql',\n",
              " 'nosql',\n",
              " 'selenium',\n",
              " 'webdriver',\n",
              " 'testng',\n",
              " 'jira',\n",
              " 'jenkins',\n",
              " 'agile',\n",
              " 'may',\n",
              " '2010',\n",
              " 'source',\n",
              " 'versioning',\n",
              " 'junit',\n",
              " 'scrum']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom NER model"
      ],
      "metadata": {
        "id": "Pl0zd0Wm9M3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('job-titles.json')\n",
        "df=df.rename(columns={'job-titles': 'text'})\n",
        "df['tag']=\"job-role\"\n",
        "\n",
        "df_companies=pd.read_csv(\"companies.csv\")\n",
        "\n",
        "rows=[]\n",
        "for i in range(len(df_companies)):\n",
        "  row = {\"text\": df_companies.loc[i, \"name\"], \"tag\":\"organization\"}\n",
        "  rows.append(row)\n",
        "df = df.append(rows, ignore_index=True)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-7olNV9o39",
        "outputId": "de230468-8ebf-434b-c4bf-21badda66e4a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    text           tag\n",
            "0                      1st grade teacher      job-role\n",
            "1                           1st pressman      job-role\n",
            "2              1st pressman on web press      job-role\n",
            "3                              21 dealer      job-role\n",
            "4                      2nd grade teacher      job-role\n",
            "...                                  ...           ...\n",
            "215079                              mapn  organization\n",
            "215080           egb systems & solutions  organization\n",
            "215081                beztak corporation  organization\n",
            "215082    inventory locator service, llc  organization\n",
            "215083  toyota material handling belgium  organization\n",
            "\n",
            "[215084 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-a6f91cae10f0>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(rows, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)  # shuffle the dataset\n",
        "X = df.text\n",
        "y = df.tag\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
        "\n",
        "tag_model = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "tag_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "y_pred = tag_model.predict(X_test)\n",
        "accuracy=accuracy_score(y_pred, y_test)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh72NBlA9vKv",
        "outputId": "9a3a3499-4f41-4e9e-e19d-56d339af8c68"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9710659269131823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extraction"
      ],
      "metadata": {
        "id": "L8A7LS6blLBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isDate(string, fuzzy=False):\n",
        "    try: \n",
        "        dateutil.parser.parse(string, fuzzy=fuzzy)\n",
        "        return True\n",
        "\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "isDate(\"18\")\n",
        "isDate(\"13th June 2022 - 5th Aug 2022\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DNM7Pgn_yIY",
        "outputId": "419ce202-cb3e-44f7-8c71-be8a7225c25c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# A complete sentence or a link cannot be a header\n",
        "def isHeader(text):\n",
        "  # A complete sentence contains at least one subject, one predicate, one object, and closes with punctuation. \n",
        "  # Subject and object are almost always nouns, and the predicate is always a verb.\n",
        "  text=nlp(text)\n",
        "  has_noun = 2\n",
        "  has_verb = 1\n",
        "  for token in text:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
        "        has_noun -= 1\n",
        "    elif token.pos_ == \"VERB\":\n",
        "        has_verb -= 1\n",
        "  if has_noun < 1 and has_verb < 1:\n",
        "    return False\n",
        "  # check for link\n",
        "  pattern=r'(?P<url>https?://[^\\s]+)'\n",
        "  links = re.findall(pattern, str(text))\n",
        "  if links:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "# splits the string at each match of: '|', '@' or at\n",
        "def partitionExperienceSubheader(string):\n",
        "  pattern = r'\\||@|\\bat\\b'\n",
        "  result = filter(None, re.split(pattern, string))\n",
        "  return list(result)\n",
        "\n",
        "test_strings = [\n",
        "    \"Resume Worded | June 2022 - Present\",\n",
        "    \"App developer at Resume Worded\",\n",
        "    \"App developer @ Resume Worded\",\n",
        "    \"App developer @Resume Worded\",\n",
        "    \"Link: https://www.geeksforgeeks.org/\",\n",
        "    \"Nurturing Lives | Feb 2020\",\n",
        "    \"K. J. Somaiya College of Engineering\",\n",
        "]\n",
        "for string in test_strings:\n",
        "    print(partitionExperienceSubheader(string))\n",
        "    print(\"Is Header: \", isHeader(string), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2070fXKX_zyP",
        "outputId": "08e0a5ee-6a2a-4e3c-fbb4-1e0785b96b64"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Resume Worded ', ' June 2022 - Present']\n",
            "Is Header:  True \n",
            "\n",
            "['App developer ', ' Resume Worded']\n",
            "Is Header:  True \n",
            "\n",
            "['App developer ', ' Resume Worded']\n",
            "Is Header:  True \n",
            "\n",
            "['App developer ', 'Resume Worded']\n",
            "Is Header:  True \n",
            "\n",
            "['Link: https://www.geeksforgeeks.org/']\n",
            "Is Header:  False \n",
            "\n",
            "['Nurturing Lives ', ' Feb 2020']\n",
            "Is Header:  False \n",
            "\n",
            "['K. J. Somaiya College of Engineering']\n",
            "Is Header:  True \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers_experience = (\n",
        "        'career profile',\n",
        "        'employment history',\n",
        "        'work history',\n",
        "        'work experience',\n",
        "        'experience',\n",
        "        'professional experience',\n",
        "        'professional background',\n",
        "        'additional experience',\n",
        "        'career related experience',\n",
        "        'related experience',\n",
        "        'programming experience',\n",
        "        'freelance',\n",
        "        'freelance experience',\n",
        "        'army experience',\n",
        "        'military experience',\n",
        "        'military background',\n",
        ")\n",
        "headers_volunteering = (\n",
        "        \"volunteer\",\n",
        "        \"volunteering\",\n",
        ")\n",
        "headers_education = (\n",
        "        'academic background',\n",
        "        'academic experience',\n",
        "        'programs',\n",
        "        'courses',\n",
        "        'related courses',\n",
        "        'education',\n",
        "        'qualifications',\n",
        "        'educational background',\n",
        "        'educational qualifications',\n",
        "        'educational training',\n",
        "        'education and training',\n",
        "        'training',\n",
        "        'academic training',\n",
        "        'professional training',\n",
        "        'course project experience',\n",
        "        'related course projects',\n",
        "        'internship experience',\n",
        "        'internships',\n",
        "        'apprenticeships',\n",
        "        'college activities',\n",
        "        'certifications',\n",
        "        'special training',\n",
        "    )\n",
        "headers_skills = (\n",
        "        'credentials',\n",
        "        'areas of experience',\n",
        "        'areas of expertise',\n",
        "        'areas of knowledge',\n",
        "        'skills',\n",
        "        \"other skills\",\n",
        "        \"other abilities\",\n",
        "        'career related skills',\n",
        "        'professional skills',\n",
        "        'specialized skills',\n",
        "        'technical skills',\n",
        "        'computer skills',\n",
        "        'personal skills',\n",
        "        'interpersonal skills'\n",
        "        'knowledge',        \n",
        "        'technologies',\n",
        "        'technical experience',\n",
        "        'proficiencies',\n",
        "        'languages',\n",
        "        'language competencies and skills',\n",
        "        'programming languages',\n",
        "        'competencies'\n",
        "    )\n",
        "headers_projects = (\n",
        "    'projects',\n",
        "    'personal projects',\n",
        "    'academic projects',\n",
        "    'freelance projects',\n",
        ")\n",
        "headers_achievements = (\n",
        "    \"other achievements\",\n",
        "    \"other accomplishments\",\n",
        "    \"previous achievements\",\n",
        "    \"previous accomplishments\",\n",
        "    \"more achievements\",\n",
        "    \"more accomplishments\"\n",
        ")\n",
        "headers_contact = (\n",
        "    \"contact\",\n",
        "    \"Reach me\",\n",
        "    \"connect\",\n",
        ")"
      ],
      "metadata": {
        "id": "UlQyyqcTvd9C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity"
      ],
      "metadata": {
        "id": "Q5ujN2p7Vq2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSimilarity(all_values, user_values, threshold, flag):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectorizer.fit(all_values)\n",
        "  filtered_values = []\n",
        "  score = 0\n",
        "\n",
        "  for skill in user_values:\n",
        "    test_word_vector = vectorizer.transform([skill])\n",
        "    similarities = cosine_similarity(test_word_vector, vectorizer.transform(all_values))\n",
        "    if similarities.max() > threshold:\n",
        "            if flag==1:\n",
        "              filtered_values.append(skill)\n",
        "            else:\n",
        "              score = score + similarities.max()\n",
        "\n",
        "  if flag==1:\n",
        "    return filtered_values\n",
        "  else:\n",
        "    return score"
      ],
      "metadata": {
        "id": "aSQZ5e7aVuYR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phrase Matcher"
      ],
      "metadata": {
        "id": "-78SGhd4rqFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMatcher(file,col,file_flag):\n",
        "  if file_flag==1:\n",
        "    df = pd.read_csv(file)\n",
        "  else:\n",
        "    df = pd.read_excel(file)\n",
        "\n",
        "  column = df[col]\n",
        "  df = pd.DataFrame(column)\n",
        "  all_values = df[col].astype(str).values.tolist()\n",
        "  all_values_lower = [x.lower() for x in all_values]\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  phrases = all_values_lower\n",
        "  matcher = PhraseMatcher(nlp.vocab)\n",
        "  patterns = [nlp.make_doc(phrase) for phrase in phrases]\n",
        "  matcher.add(\"PHRASES\", None, *patterns)\n",
        "  return matcher\n",
        "\n",
        "def getMatchedPhrases(doc,matcher):\n",
        "  matches = matcher(doc)\n",
        "  values=[]\n",
        "  for match_id, start, end in matches:         \n",
        "      matched_phrase = doc[start:end]\n",
        "      values.append(str(matched_phrase))\n",
        "  return values\n"
      ],
      "metadata": {
        "id": "cavCI71Qrq9w"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumeParser:\n",
        "\n",
        "  def __init__(self, text):\n",
        "    self.resume_text = text\n",
        "    self.lines = text.split(\"\\n\")\n",
        "  \n",
        "  resume_sections = {\n",
        "      'name': {},\n",
        "      'experience': {},\n",
        "      'volunteering': {},\n",
        "      'education': {},\n",
        "      'skills': {},\n",
        "      'projects': {},\n",
        "      'achievements': {},\n",
        "      'contact': {}\n",
        "  }\n",
        "  header_indices = {}\n",
        "\n",
        "  #Obtain line nos of Headers \n",
        "  def getHeaderIndices(self):\n",
        "    for i, line in enumerate(self.lines):\n",
        "      if len(line):\n",
        "        #Header always starts in Uppercase\n",
        "        if line[0].isupper():\n",
        "          #The line is a header if it matches the required section headers and isn't a complete sentence\n",
        "          if [item for item in headers_experience if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"experience\"\n",
        "          elif [item for item in headers_volunteering if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"volunteering\"\n",
        "          elif [item for item in headers_education if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"education\"\n",
        "          elif [item for item in headers_skills if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"skills\"\n",
        "          elif [item for item in headers_projects if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"projects\"\n",
        "          elif [item for item in headers_achievements  if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"achievements\"\n",
        "          elif [item for item in headers_contact if line.lower().startswith(item)] and isHeader(line):\n",
        "            self.header_indices[i]=\"contact\"\n",
        "\n",
        "  #Obtain raw text for different resume sections using header indices/linenos\n",
        "  def getRawResumeSections(self):\n",
        "    no_lines = len(self.lines)\n",
        "    header_linenos = list(self.header_indices.keys())\n",
        "    list_last_index = len(header_linenos)-1\n",
        "    for counter, lineno in enumerate(header_linenos):\n",
        "      start_index = lineno+1\n",
        "      if (counter<list_last_index):\n",
        "        end_index = header_linenos[counter+1]\n",
        "      else:\n",
        "        end_index = no_lines\n",
        "      self.resume_sections[self.header_indices[lineno]]=self.lines[start_index:end_index]\n",
        "\n",
        "  def getExperience(self, section_string):\n",
        "    if not bool(self.resume_sections[section_string]):\n",
        "      return\n",
        "    text = self.resume_sections[section_string]\n",
        "    self.resume_sections[section_string] = [] #list of dictionaries for details of experiences\n",
        "    exp={\"description\":\"\"}\n",
        "    go_to_next=False\n",
        "    for i, line in enumerate(text):\n",
        "      if len(line):\n",
        "        if line[0].isupper():\n",
        "          # if line isn't a complete sentence, it can be either org, job role or date\n",
        "          if isHeader(line):\n",
        "            line_parts = partitionExperienceSubheader(line)\n",
        "            for line_part in line_parts:\n",
        "              if isDate(str(nlp(line_part)[0:2])):\n",
        "                exp['duration']=line_part\n",
        "              else:\n",
        "                if go_to_next:\n",
        "                  self.resume_sections[section_string].append(exp) #add to list before moving to next\n",
        "                  exp={\"description\":\"\"}\n",
        "                  go_to_next=False\n",
        "                # if not a date, apply model to predict tag\n",
        "                exp[tag_model.predict([line_part])[0]]=line_part\n",
        "          else:\n",
        "            exp[\"description\"]+=line\n",
        "            go_to_next=True #Exp description is written at last, after which next experience appears\n",
        "        else:\n",
        "            exp[\"description\"]+=line\n",
        "            go_to_next=True\n",
        "    self.resume_sections[section_string].append(exp)\n",
        "\n",
        "  \n",
        "  def getEducation(self):\n",
        "      parsed_resume_edu=[]\n",
        "  \n",
        "      matcher_b = getMatcher('Major_or_Branch.csv','Major',1)\n",
        "      matcher_d = getMatcher('Academic Degrees.xlsx','Degree',2)\n",
        "\n",
        "      # Test the matcher\n",
        "      for edu in self.resume_sections[\"education\"]:\n",
        "        \n",
        "        parsed_branch_degree={}\n",
        "        edu = edu.lower()\n",
        "        edu_new = edu.replace(\".\", \"\")\n",
        "\n",
        "        doc = nlp(edu_new)\n",
        "        \n",
        "        degrees = getMatchedPhrases(doc,matcher_d)\n",
        "        if len(degrees)!=0:\n",
        "          parsed_branch_degree[\"degree\"] = degrees\n",
        "\n",
        "        branches = getMatchedPhrases(doc,matcher_b)\n",
        "        if len(branches)!=0:\n",
        "          parsed_branch_degree[\"branch\"] = branches\n",
        "       \n",
        "\n",
        "        if len(parsed_branch_degree)!=0:\n",
        "          parsed_resume_edu.append(parsed_branch_degree)    \n",
        "      return parsed_resume_edu      \n",
        "\n",
        "  #get skills from resume\n",
        "  def getSkills(self):\n",
        "        file ='Technology Skills.xlsx'\n",
        "        df = pd.read_excel(file)\n",
        "        column = df['Example']\n",
        "        df = pd.DataFrame(column)\n",
        "        all_skills = df['Example'].astype(str).values.tolist()\n",
        "        all_skills_lower = [x.lower() for x in all_skills]\n",
        "\n",
        "        resume_skills =\"\"\n",
        "\n",
        "        for skill in self.resume_sections[\"skills\"]:\n",
        "          resume_skills =  resume_skills + \" \"+skill\n",
        "\n",
        "        resume_skills =  resume_skills.encode(\"ascii\", \"ignore\")\n",
        "        resume_skills =  resume_skills.decode()\n",
        "        resume_skills_tokens = preProcessess(resume_skills)\n",
        "      \n",
        "        return getSimilarity(all_skills_lower, resume_skills_tokens, 0.6, 1) \n",
        "\n",
        "  #method to parse the entire resume - calls all the above methods of the class\n",
        "  def getResumeData(self):\n",
        "    self.getHeaderIndices()\n",
        "    if len(self.header_indices)!=0:\n",
        "      self.getRawResumeSections()\n",
        "      self.getExperience(\"experience\")\n",
        "      self.getExperience(\"volunteering\")\n",
        "      self.parsed_resume_education=self.getEducation()\n",
        "      self.parsed_resume_skills = self.getSkills()\n",
        "    return self.resume_sections"
      ],
      "metadata": {
        "id": "QBScRJMcvd9D"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = ResumeParser(resume_text)\n",
        "obj.getResumeData()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9ff336-c06f-4515-ec77-f8e132b18257",
        "id": "vSBljPMwvd9D"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(obj.resume_sections[\"experience\"], indent=4, sort_keys=True))\n",
        "print(json.dumps(obj.resume_sections[\"volunteering\"], indent=4, sort_keys=True))\n",
        "print(json.dumps(obj.parsed_resume_education, indent=4, sort_keys=True))\n",
        "print(json.dumps(obj.parsed_resume_skills, indent=4, sort_keys=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2GB9PPVAdSy",
        "outputId": "a1d6e8cf-45d0-4891-8bb0-3c0ff9180e63"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"description\": \" Enabled critical test case complexity metrics with support for Rapid adoption of  functional automation usinga scriptless test case adaptor by standardizing a Test Case construction method that was builtAutomation-ready & supported a test automation framework leading to a 45% increase in reusability withreductions in TCO approaching 25%. Optimized scripting, modularity, & maintenance which resulted in an 18% decrease in workflow friction. Increased the companys ability to take and complete projects without increasing manpower by 15% byreducing QA testing turnaround time by 30%.\",\n",
            "        \"duration\": \"Jun 2018  Present\",\n",
            "        \"job-role\": \"QA Manual Tester\",\n",
            "        \"organization\": \"Resume Worded, New York, NY\"\n",
            "    },\n",
            "    {\n",
            "        \"description\": \" Restructured utilities & improved the process documentation leading to a 40% reduction in client supporttickets & an 80% increase in uptime. Achieved department-wide improvement metrics based on QA scorecard through the 46% & 22% workloadreduction of  the customer support & IT departments respectively. Standardized Test Plan, Test Scripts/Test Cases, Daily Status Reports, etc., documents leading to a 20%increase in productivity. Established monthly sprint backlog items as well as performed agile meetings while updating the activities inMicrosoft TFS in an optimized manner which resulted in saving 10 hours of  monthly lost time.\",\n",
            "        \"duration\": \"Jan 2015  May 2018\",\n",
            "        \"job-role\": \"QA Manual Tester\",\n",
            "        \"organization\": \"Growthsi, New York, NY\"\n",
            "    },\n",
            "    {\n",
            "        \"description\": \" Optimized the build process by increasing the systems quality level and reducing 45% of  defects found. Established proper team communication that identified, triaged, reproduced, & fixed found issues using JIRAincreasing the overall workflow by 25%.\",\n",
            "        \"duration\": \"May 2008  Dec 2014\",\n",
            "        \"job-role\": \"QA Manual Tester (Nov 2011  Dec 2014)\",\n",
            "        \"organization\": \"RW Capital, San Diego, CA\"\n",
            "    },\n",
            "    {\n",
            "        \"description\": \" Wrote & optimized test scripts in towels which led to a 9% reduction in the overall testing hours. Created traceability matrix to fill in the gap between requirements and tests covered contributing to the 10%increase in test case count.\",\n",
            "        \"job-role\": \"Junior QA Manual Tester (May 2010  Oct 2011)\"\n",
            "    }\n",
            "]\n",
            "{}\n",
            "[\n",
            "    {\n",
            "        \"branch\": [\n",
            "            \"computer science\"\n",
            "        ],\n",
            "        \"degree\": [\n",
            "            \"bsc\"\n",
            "        ]\n",
            "    }\n",
            "]\n",
            "[\n",
            "    \"test\",\n",
            "    \"automation\",\n",
            "    \"java\",\n",
            "    \"javascript\",\n",
            "    \"sql\",\n",
            "    \"nosql\",\n",
            "    \"selenium\",\n",
            "    \"testng\",\n",
            "    \"jira\",\n",
            "    \"jenkins\",\n",
            "    \"source\",\n",
            "    \"junit\"\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Resume skills with JD skills"
      ],
      "metadata": {
        "id": "9wAU-4kSWbnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#JD with each skill on new line in text file\n",
        "\n",
        "with open('JD_Skills.txt', 'r') as file:\n",
        "    jd_skills_tokens = [line.strip() for line in file]\n",
        "\n",
        "jd_skills_tokens = [i for i in jd_skills_tokens if i] #remove empty strings\n",
        "jd_skills_tokens = [x.lower() for x in jd_skills_tokens]\n",
        "\n",
        "resume_score =  getSimilarity(jd_skills_tokens, obj.parsed_resume_skills,0.5,2)\n",
        "\n",
        "matching_per = (resume_score/len(jd_skills_tokens))*100\n",
        "\n",
        "print(\"Skills Matching % with JD: \"+str(matching_per))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7dpX0mMX2_K",
        "outputId": "3a62f4ff-91cd-4420-8289-cc1b6e557fec"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skills Matching % with JD: 31.547005383792516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting output to HTML\n",
        "with open('output.html', 'a') as f:\n",
        "    print_list = []\n",
        "    print_list.append(pd.DataFrame(obj.resume_sections[\"experience\"], columns=['organization', 'job-role', 'duration', 'description']))\n",
        "    print_list.append(pd.DataFrame(obj.resume_sections[\"volunteering\"], columns=['organization', 'job-role', 'duration', 'description']))\n",
        "    for item in print_list:\n",
        "      if not item.empty:\n",
        "        html_table_blue_light = build_table(item, 'blue_light')\n",
        "        f.write(html_table_blue_light)"
      ],
      "metadata": {
        "id": "ehp99nbCAYtT"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}